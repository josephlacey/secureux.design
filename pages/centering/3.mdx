import { MDXProvider } from "@mdx-js/react";
import { useAppContext as uac } from "components/AppProvider";
import { colors } from "styles/theme";

export default function Centering3({ children }) {
  return (
    <MDXProvider
      components={uac().components(
        colors.red,
        colors.darkRed,
        uac().useMobile()
      )}
    >
      {children}
    </MDXProvider>
  );
}

[comment]: # "↓↓↓ Editable ↓↓↓"

###### 20-30min

##### Intermediate Level | Individual quiz activity and group discussion | Pen/Paper, Post-its, Computer Text File

1. Phase 1: Centering Human Rights
2. Chapter 3

# Understanding Harm (Threat Modeling)

## Understanding the Threats to Your Communities

In the previous chapter we saw that to center human rights, we must center the safety of the communities we are working with. However, to truly employ human rights centered design, we also must ensure that the decisions we make with and for our communities don’t have harmful effects for others downstream. This chapter focuses on threat modeling, which allows us to map out any potential harm our communities might face, while also thinking about the ripple effects and potential harm that other communities can face once we’ve created the product or service. It’s important we take an intersectional approach towards understanding harm mitigation, and security, across all our different processes.

When you are working on a product or service with vulnerable communities, before starting your research, you will first need to assess the risks the communities face or create a threat model to help understand where they are vulnerable to attacks, physically, or digitally. A great place to begin your threat-modeling is [Tactical Tech’s Holistic Security Guide: A Strategy Manual
for Human Rights Defenders](https://holistic-security.tacticaltech.org/chapters/explore.html). To exemplify the why and how of best practices in this scenario, we will share real-world use cases and include guest experts’ on-the-ground experiences that exemplify the processes we use to ensure the efficacy and safety of our communities, their data, and those they work with during our research processes.

## Considerations of privacy and security tools

Creating a safe environment goes beyond the commonly known privacy and security issues we often see today. Privacy-focused tools can fail, and safeguards and considerations of downstream effects need to be in place for when this happens in order to prepare. When it comes to building an app, guide, or making software, the creators must understand how that product might exclude people or make assumptions about the user base which can lead to harm. We are covering a lot of ground in this chapter.

One example of how a security tool failed to protect its users is when the mobile chat application Telegram was compromised. The breach led to the data of 500 million users being leaked online. While Telegram employs some security measures, it was not end-to-end encrypted and private data of its users was unknowingly made public. Check Point is a software technology solutions company that provides cyber security solutions to governments and corporations globally. [They issued a warning against Telegram](https://www.forbes.com/sites/zakdoffman/2021/04/22/forget-whatsapp-new-telegram-warning-for-millions-of-windows-10-users/?sh=7c85207a7857) saying that it has “tracked 130 cyber attacks that used malware managed over Telegram by attackers in the last three months... Even when Telegram is not installed or being used, it allows hackers to send malicious commands and operations remotely via the instant messaging app.” The actual malware is sophisticated and sent through email messages where it connects with the bots in the Telegram app. While Telegram purports to be a ‘security-focused’ chat app, in reality, it wasn’t secure enough for the communities who prioritized privacy and digital freedom. One way to help your communities stay safe is to look at the [Electronic Frontier Foundation's most up to date Surveillance Self Defense Guide](https://ssd.eff.org/en) and choose vetted tools to use when creating closed-loop, secure communications channels.

## What is threat modeling?

Threat modeling is [“a way of narrowly thinking about the sorts of protection you want for your data. It's impossible to protect against every kind of trick or attacker, so you should concentrate on which people might want your data, what they might want from it, and how they might get it. Coming up with a set of possible attacks you plan to protect against is called threat modeling.”](https://sec.eff.org/topics/threat-modeling) Once you have a threat model, you can conduct a risk analysis. The more at-risk the community is, the more preparation you will likely need to do. We will highlight different scenarios to help guide you through real-world threat-modeling and research practices that security experts and human rights defenders use. Our goal is to increase awareness about the most ethical, cooperative, and safe ways to do this work, so it sets you up for the best outcomes: ensuring the safety of those you are building with and for, and producing a product or service that truly meets the needs of your community. There are three sub-phases within the research process that we will cover: threat-modeling, setting up communication channels and practices to protect the community during your research, and re-checking your process to ensure these practices are ethical and do not inadvertently put the community at risk.

One key thing to consider is that when incorporating threat modeling into your preparation for working with a community, you may quickly realize that it is safer not to move forward on the project. This would happen if you realized your work puts individuals at higher risk during the research, iteration, or implementation phases than it will help mitigate. This is valuable information that can help you assess whether you will be able to ethically work with communities or, if not, what kind of communities you are better prepared to work with.

Here we will focus specifically on considerations human rights practitioners consider during the research and ideation stages to ensure the safety, efficacy, usability, and sustainability of their products and services. Before beginning your research, it is useful to incorporate these three practices into your preparation:

- Learning the unique threat models your communities face;
- Adapting your practices to mitigate these threats through behavioral and technical approaches (ex: setting up secure communication channels or choosing to meet in person without internet-connected devices); and
- Ensuring the transparency of and deep involvement or inclusion in your process with the communities you work with.

## Why do this kind of process?

There are countless stories of the harms for not centering vulnerable communities and marginalized groups, beyond making useless technology that actively harms users. One particular example of this is the ‘real names’ policy. Researcher and policy expert [Jillian C. York has acutely pointed out that every few years a white man on the internet thinks they have come up with the solution to ‘solving online harassment’ which is to require users to use their real names.](https://jilliancyork.com/2021/01/14/everything-old-is-new-part-2-why-online-anonymity-matters/) But, as many BIPOC and queer researchers, journalists and activists have pointed out, a ‘real names’ policy further harms marginalized groups and does not protect them. Early in 2014, Facebook tried to implement a real names policy. It failed to address questions like, what is a real name? How could someone verify this? What sounds ‘real’ and what doesn’t? As a result, [Facebook's algorithm couldn’t recognize the names of members of the LGBTQIA+ community and drag performers](https://www.eff.org/de/deeplinks/2014/09/facebooks-real-name-policy-can-cause-real-world-harm-lgbtq-community), [along with other specific groups such as Indigenous names of Native Americans](https://tanginstitute.andover.edu/files/Book-Review_-Technically-Wrong-1.pdf). However, more importantly, we’ve noticed in our own ethnographic research on online harassment, that some harassers will use their real names. In Caroline’s research into far-right communities on Twitter and Facebook, she repeatedly saw users using their real names. If the goal of the various platforms is to deter harm and create accountability, the platforms failed because they created more harm against the very communities that needed protecting. In this case, some basic user research and threat modeling on the design proposal of ‘using real names’ could have informed the platforms as to why this was not an adequate solution.

What can we learn from this? We must work with many different communities while having a deeper understanding of harm across digital and physical spaces. One of our solutions or products or recommendations to solve something shouldn’t come at the expense of harming another group.

### Step 1: Threat Modeling: What to Consider When Preparing to Start Your Research

No matter who or where you are in the world, there are various threats to consider when working with others. The expansion of surveillance technology, the internet of things, and potentially malicious algorithms are creating new threats for anyone who uses the internet. If you are working with those who are vulnerable or marginalized, it is even more important that you can prevent exacerbation of these current issues and even more dangerous threats to them. As we’ve seen, [doxxing](https://sectigostore.com/blog/what-is-doxxing-5-examples-of-doxxing-and-how-to-prevent-it/) is one example of how bad actors can reveal personal information online without the target’s consent.

Once you begin to assess your community’s threat model, you may see patterns of where they are most vulnerable. For example, widely used apps like WhatsApp and Telegram are purportedly “secure” but have both been compromised by various actors. In the case of Telegram, it has previously been compromised by hackers who infiltrated the platform and private data. [Whereas WhatsApp succumbed to its parent company Facebook when it changed the terms of service forcing users to share their data with Facebook on February 8th, 2021.](https://www.forbes.com/sites/zakdoffman/2021/01/09/stop-using-whatsapp-after-facebook-apple-imessage-and-signal-privacy-backlash/?sh=13e364e86cf5)

Given our global networks, many individuals want secure platforms to connect with family members, colleagues, or friends across spaces so securing your communications is a great way to begin before starting your research. Even if you don’t deem the groups to be at high risk, research between an organization and community should be protected for the best interest of both parties.

### Step 2: Setting up Secure Communications to Mitigate Threats of Surveillance

While not all communities are being surveilled, most at-risk communities need an extra layer of privacy to prevent future harms as they are likely already being targeted. While privacy tools are increasingly easier to use, it always takes time to adapt them into your workflow. When it comes to the security of your community, the small investment of time to do this right is one of the most important steps you can take to protect them from harm.

- **In-person**: If you are meeting in person, ask participants to turn their phones off and take notes on paper

- **Chat / Group Chat / Video / Phone Calls**: If you need a secure chat app that does almost everything you need to connect with many folks in real time, the [Signal app](https://signal.org/) is currently the gold standard because it is not only open source, but also provides end-to-end encryption and has no centralized server hosting your data on it (it is only hosted locally on your phone or desktop app). If you are unable to use Signal, look for the same features in other apps.

- **Email**: PGP (or pretty good privacy) is very secure but equally as difficult to use, so if you are unable to set that up with your community, many email providers now offer secure email that have security features like end-to-end encryption, 2FA, and a reputation that doesn’t include serious security vulnerabilities or breaches. Look out for up-to-date recommendations from the open source community such as [It’s Foss’ 2021 guide](https://itsfoss.com/secure-private-email-services/) (which will surely age quickly, so make sure you keep an eye on updates!

- **Collaborative Documents**: Keep an eye out for tools that offer encryption and decryption in your browser. This means that there is no centralized database containing your information from documents, chats, and files, which are unreadable outside of the session where you are logged in. Tools such as [Cryptpad](http://Cryptpad.fr) currently offer these features.

### Step 3: Work Equitably:

Ensure the transparency of and deep involvement or inclusion in your process with the communities you work with. This is set in place to mitigate internal harms that could arise during your processes. And yes, internal threats can be just as, if not more, harmful than external threats!

- **Transparency and Inclusion**: Create shared documents and communications channels that everyone can access and edit, ensure your decision-making process is agreed to by all stakeholders and there are clear guidelines and safeguards to protect that process if it is violated.

- **Create a Code of Conduct**: Create a code of conduct for your organization that you share with the communities you work with, so everyone knows what types of behaviors are acceptable and which are not. A great example of an CoC for open source projects is by the Contributor’s Covenant. While it is geared towards open source projects, it can be extrapolated to the ways stakeholders interact more generally.

## Putting Threat Modeling into Action for Research

There are many other reasons to include communities in your design, research and building processes. For example, we’ve seen different ‘solutions’ pop in an attempt to stop different kinds of abuse and misuse that puts other groups in harm’s way. In early August 2021, Apple announced a new plan to combat child pornography: they would scan images on every phone. However, privacy experts quickly pointed out that Apple’s approach to this scanning of people’s images on their own devices could give law enforcement, and governments new ways to surveil people, including journalists, activists, and dissidents. Why would this be an issue, though? Once a backdoor is built, anyone can kick it in. While Apple promised it would not grant access to individual’s phones or bow from governmental pressure. However, one month later in September 2021, Apple and Google bowed to pressure from the Russian government. The online news site, the Verge reported that both “Apple and Google have removed jailed Russian opposition leader Alexei Navalny’s voting app from the iOS and Android stores under pressure from the government. [The New York Times reports](https://www.nytimes.com/2021/09/17/world/europe/russia-navalny-app-election.html) that the removal followed threats to criminally prosecute company employees within Russia.”

In another example, sometimes companies require state or country identification cards to help prove the identity of a user. These cards contain highly sensitive data and if obtained or used in a malicious way by others, can have grave consequences to the holder. Can the companies that request the cards for verification guarantee there won’t be a data leak or that we can accurately or safely store this sensitive information? What would happen if this database of IDs were released into the world? What harm would that cause? Instead of requiring real IDs, we should be asking if the company really needs ID or if we need verification? Are there other kinds of verification we can require instead? Truthfully, it depends on what the product or service is. We are more likely to justify it if a governmental app to verify a COVID19 vaccine requires an ID. But what if your app is just for accessing email? What can you use instead? You can use 2 factor authentication, a YUBI key, or a google authenticator to ensure that whoever is logging into that email account is the right person to be logging in without having to submit sensitive information.

These two examples may seem disparate, but they are linked. The takeaway here is that it’s hard to ensure safety without having safe and secure devices. As users, builders, designers, and researchers, we shouldn’t assume, or trust, that a platform can work with our best interests at heart. Therefore it’s so key to have privacy focused design and technology.

The big takeaway is understanding that when you and your community work together to create tools or services, there are downstream and pluralistic, global effects that ripple across many different communities that may use the same tools or resources to mitigate different problems. Assessing ways that your work can be abused in different environments is a necessary step in the threat modeling process so that you don’t inadvertently cause serious harm to another group. If this is the case, you will need to modify your work to address those threats.

## Learn how to assess the threats your community faces by learning more about an incident after it has occurred:

Ask the following questions to gain more insight about abuse or harassment someone has faced:

- What happened?
- What devices did it happen on (if any)?
  - This can help you determine if other devices were affected or how to patch the vulnerability, etc.
- Does anyone else have physical or remote access to your device that this happened on?
- What time did the event occur?
  - This helps you contextualize the event and possibly learn why this happened (i.e. was it after the person tweeted something publicly? Attended an event? After a mass email leak? etc.)
- What information did you see in the attack (like a popup/screen/text/image/etc.)?
  - This is useful because you can often google the exact language to learn more about the exploit
- How often does this happen?

Once you’ve answered these questions, you can create a safety plan, a harassment response plan, and an investigation plan, in order to figure out how to get help and prevent further harm.

![alt text](/images/uxs-icon-1.svg)
